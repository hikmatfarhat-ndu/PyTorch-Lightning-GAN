{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81e6d26a8834f1381eedfc951ade80c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a11edaddaef24ca58fba99c3b9298772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c57dbe5fb984592aa667f2c2c8c157b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c575b024d1644f7face68220f0c54a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnist_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "                                      transforms.Lambda(lambda x: x.view(-1, 784)),\n",
    "                                      transforms.Lambda(lambda x: torch.squeeze(x))\n",
    "                                      ])\n",
    "\n",
    "data = datasets.MNIST(root='.', download=True, transform=mnist_transforms)\n",
    "\n",
    "mnist_dataloader = DataLoader(data, batch_size=128, shuffle=True, num_workers=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "  '''\n",
    "  Generator class in a CGAN. Accepts a noise tensor (latent dim 100)\n",
    "  and a label tensor as input as outputs another tensor of size 784.\n",
    "  Objective is to generate an output tensor that is indistinguishable \n",
    "  from the real MNIST digits.\n",
    "  '''\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding(10, 10)\n",
    "    self.layer1 = nn.Sequential(nn.Linear(in_features=100+10, out_features=256),\n",
    "                                nn.LeakyReLU())\n",
    "    self.layer2 = nn.Sequential(nn.Linear(in_features=256, out_features=512),\n",
    "                                nn.LeakyReLU())\n",
    "    self.layer3 = nn.Sequential(nn.Linear(in_features=512, out_features=1024),\n",
    "                                nn.LeakyReLU())\n",
    "    self.output = nn.Sequential(nn.Linear(in_features=1024, out_features=28*28),\n",
    "                                nn.Tanh())\n",
    "\n",
    "  def forward(self, z, y):\n",
    "    # pass the labels into a embedding layer\n",
    "    labels_embedding = self.embedding(y)\n",
    "    # concat the embedded labels and the noise tensor\n",
    "    # x is a tensor of size (batch_size, 110)\n",
    "    x = torch.cat([z, labels_embedding], dim=-1)    \n",
    "    x = self.layer1(x)\n",
    "    x = self.layer2(x)\n",
    "    x = self.layer3(x)\n",
    "    x = self.output(x)\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Discriminator(nn.Module):\n",
    "  '''\n",
    "  Discriminator class in a CGAN. Accepts a tensor of size 784 and\n",
    "  a label tensor as input and outputs a tensor of size 1,\n",
    "  with the predicted class probabilities (generated or real data)\n",
    "  '''\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding(10, 10)\n",
    "    self.layer1 = nn.Sequential(nn.Linear(in_features=28*28+10, out_features=1024),\n",
    "                                nn.LeakyReLU())\n",
    "    self.layer2 = nn.Sequential(nn.Linear(in_features=1024, out_features=512),\n",
    "                                nn.LeakyReLU())\n",
    "    self.layer3 = nn.Sequential(nn.Linear(in_features=512, out_features=256),\n",
    "                                nn.LeakyReLU())\n",
    "    self.output = nn.Sequential(nn.Linear(in_features=256, out_features=1),\n",
    "                                nn.Sigmoid())\n",
    "    \n",
    "  def forward(self, x, y):\n",
    "    # pass the labels into a embedding layer\n",
    "    labels_embedding = self.embedding(y)\n",
    "    # concat the embedded labels and the input tensor\n",
    "    # x is a tensor of size (batch_size, 794)\n",
    "    x = torch.cat([x, labels_embedding], dim=-1)    \n",
    "    x = self.layer1(x)\n",
    "    x = self.layer2(x)\n",
    "    x = self.layer3(x)\n",
    "    x = self.output(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz=100\n",
    "device=torch.device(\"cuda\" if(torch.cuda.is_available()) else \"cpu\")\n",
    "G=Generator().device(device)\n",
    "D=Discriminator().device(device)\n",
    "# Initialize the ``BCELoss`` function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\min_G\\max_D V(D,G)=\\mathbb{E}_{x\\sim p_{data}}[\\log D(x)]+\\mathbb{E}_{z\\sim p_z}[\\log(1-D(G(z)))]\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, (x, labels) in enumerate(mnist_dataloader):\n",
    "    # Sample random noise and labels\n",
    "    z = torch.randn(x.shape[0], 100, device=device)\n",
    "    y = torch.randint(0, 10, size=(x.shape[0],), device=device)\n",
    "    # Generate images\n",
    "    generated_imgs = G(z, y)\n",
    "    # Calculate the discriminator output for fake images\n",
    "    d_out_fake = D(generated_imgs, y)\n",
    "    # Calculate the discriminator output for real images\n",
    "    d_out_real = D(x, y)\n",
    "\n",
    "    # discriminator loss  \n",
    "    d_loss = criterion(d_out_fake, torch.zeros_like(d_out_fake))\\\n",
    "             +criterion(d_out_real, torch.ones_like(d_out_real)) \n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
